<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>KubeSphere | 面向云原生应用的容器混合云 on </title>
    <link>https://kubesphere.eu/zh/</link>
    <description>Recent content in KubeSphere | 面向云原生应用的容器混合云 on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    
	<atom:link href="https://kubesphere.eu/zh/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>aaaaaa</title>
      <link>https://kubesphere.eu/zh/docs/quick-start/second/third/aaa/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/docs/quick-start/second/third/aaa/</guid>
      <description>test test</description>
    </item>
    
    <item>
      <title>quick-start-guide</title>
      <link>https://kubesphere.eu/zh/docs/quick-start/quick-start-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/docs/quick-start/quick-start-guide/</guid>
      <description>test test</description>
    </item>
    
    <item>
      <title>what-is-kubesphere</title>
      <link>https://kubesphere.eu/zh/docs/introductions/what-is-kubesphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/docs/introductions/what-is-kubesphere/</guid>
      <description>test test</description>
    </item>
    
    <item>
      <title>all-in-one</title>
      <link>https://kubesphere.eu/zh/docs/introductions/all-in-one/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/docs/introductions/all-in-one/</guid>
      <description>test test</description>
    </item>
    
    <item>
      <title>second-a</title>
      <link>https://kubesphere.eu/zh/docs/quick-start/second/a/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/docs/quick-start/second/a/</guid>
      <description>test test</description>
    </item>
    
    <item>
      <title>Anchnet</title>
      <link>https://kubesphere.eu/zh/case/anchnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/case/anchnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Aqara</title>
      <link>https://kubesphere.eu/zh/case/aqara/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/case/aqara/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benlai</title>
      <link>https://kubesphere.eu/zh/case/benlai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/case/benlai/</guid>
      <description></description>
    </item>
    
    <item>
      <title>contribution request</title>
      <link>https://kubesphere.eu/zh/contribution/request/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/contribution/request/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Huaxia Bank</title>
      <link>https://kubesphere.eu/zh/case/huaxia-bank/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/case/huaxia-bank/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KubeSphere 2.1.1 发布！全面支持 Kubernetes 1.17！多项功能与用户体验优化！</title>
      <link>https://kubesphere.eu/zh/blogs/release-210/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/release-210/</guid>
      <description>2.1.0 正式发布 2019 年 11 月 11 日，KubeSphere 开源社区激动地向大家宣布，KubeSphere 2.1.0 正式发布！2.1.0 版本不仅在安装上提供了最快速方便的安装方式，解耦了核心的功能组件并提供了可插拔的安装方式，还提供了非常多的让开源社区用户期待已久的新功能，并修复了已知的 Bug。
同时，社区对 KubeSphere 组件的高可用进行了深度优化与测试，因此，该版本也是被定义为 Prodcution-ready 的，支持用户在生产环境部署和使用。我们在此对社区用户提交的 issue、PR、Bug 反馈、需求建议、文档改进等一系列贡献表示由衷的感谢，并对 2.1.0 版本做出巨大贡献的开发者们深表谢意。
在新版本中，KubeSphere 对 安装部署、DevOps、应用商店、存储、可观察性、认证与权限 等模块提供了诸多新功能和深度优化，更好地帮助企业用户在测试生产环境快速落地云原生技术和运维 Kubernetes，使开发者能够更专注在业务本身，赋能运维和测试人员高效地管理集群资源，实现业务快速发布与持续迭代的需求。同时，功能组件的可插拔安装能够满足不同用户的个性化需求，下面先通过一张图来快速介绍 2.1.0 版本各功能模块的新功能与优化项。
应用商店 KubeSphere 是一个 以应用为中心 的容器平台，基于自研的开源项目 OpenPitrix (openpitrix.io) 构建了应用商店、内置应用仓库与应用生命周期管理，KubeSphere 应用商店 对内可作为团队间共享企业内部的中间件、大数据、业务应用等，以应用模板的形式方便用户快速地一键上传和部署应用到 Kubernetes 中；对外可作为根据行业特性构建行业交付标准、交付流程和交付路径的基础，作为行业通用的应用商店，可根据不同需求应对不同的业务场景。
在 2.1.0 版本中，KubeSphere 从业务视角实现了应用的生命周期管理，支持 Helm 应用的 上传提交、应用审核、测试部署、应用上架、应用分类、应用升级、应用下架，帮助开发者或 ISV 将应用共享和交付给普通用户。同时，应用商店内置了多个常用的 Helm 应用方便开发测试。未来将提供基于应⽤的监控指标、应⽤⽇志关键字段告警能⼒，以及计量计费等运营功能。
DevOps DevOps 是云原生时代在开发测试与持续交付场景下最核心的一环，KubeSphere 2.1.0 对 DevOps 系统进行了深度优化，流水线、S2I、B2I 提供了代码依赖缓存支持，使构建速度大幅提升。在 CI/CD 流水线集成了更多 Jenkins 插件和版本，优化了流水线 Agent 节点选择，新增了对 PV、PVC、Network Policy 的支持，并将这一系列优化成果贡献给了 Jenkins 社区。</description>
    </item>
    
    <item>
      <title>KubeSphere 前端开源，社区架构首次公布</title>
      <link>https://kubesphere.eu/zh/blogs/console-opensource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/console-opensource/</guid>
      <description>Console 开源 从 KubeSphere 第一行代码至今，项目经历了一年多时间的迅速发展，开源社区也这个期间完成孵化，并初具规模。为了让 KubeSphere 项目能够更好地以开源社区的形式发展和演进，让社区开发者能够方便地参与到 KubeSphere 项目的建设，社区宣布将 KubeSphere 前端项目 console 开源，当前开源的版本包含了 KubeSphere 最新发布 v2.1 所有功能的代码，前端的 Feature Map 可以通过这张图快速了解。
Feature Map
前端项目的代码已在 github.com/kubesphere/console 可见，欢迎大家 Star + Fork。此前已有多位社区用户与开发者表示，希望能参与到 KubeSphere 项目的前端贡献，现在大家已经可以从 github.com/kubesphere/kubesphere/issues 通过标签 area/console 找到前端相关的 issue，包括 Bug、feature and design。
至此，KubeSphere 开源社区发布的项目已涵盖了容器平台（KubeSphere）、多云应用管理平台（OpenPitrix）、网络插件（Porter LB 插件、Hostnic-CNI）、存储插件（CSI）、CI/CD（S2i-operator）、日志插件（Fluentbit-operator）、通知告警（Alert &amp;amp; Notification）、身份认证（IAM）等。
KubeSphere 社区架构 KubeSphere 相信 Community over code，一个健康良好的开源社区发展必定离不开 Contributor 的参与。为了让社区相关的事情更加成体系，让社区同学更有归属感，KubeSphere 首次建立了社区架构，第一次公开的架构包括 Developer Group 和 User Group。
 SIG（特别兴趣小组）由开发者和用户共同组成，目前架构中暂未划分主题，未来将根据社区用户的参与和关注方向进行划分。
 Developer Group Developer Group 将以开发者对 KubeSphere 组织下的所有开源项目的贡献数量和质量作为参考，可贡献的项目包括前后端、存储与网络插件、官网文档等项目。
 Active Contributor：2 个月内贡献过超过 4 个 PR，这样即可获得邀请。 Reviewer： 从 Active Contributor 中诞生，当 Active Contributor 对该模块拥有比较深度的贡献，并且得到 2 个或 2 个以上 Maintainer 的提名时，将被邀请成为该模块的 Reviewer，具有 Review PR 的义务。 Maintainer：即该功能模块的组织者，负责项目某个功能模块的代码与版本开发与维护，社区日常运营，包括组织会议，解答疑问等。Tech Lead 需要为项目的管理和成长负责，责任重大。目前暂由 KubeSphere 内部成员担任，将来可根据贡献程度由社区开发者一起担任，共同为项目的进步而努力。  User Group KubeSphere 社区是由开发者和用户共建的，随着 KubeSphere 用户群体愈发壮大，用户在使用过程中遇到的问题反馈及实践经验，对于 KubeSphere 产品的完善及应用推广有着不可忽视的重要作用。</description>
    </item>
    
    <item>
      <title>KubeSphere 容器平台发布 2.1.1，全面支持 Kubernetes 1.17</title>
      <link>https://kubesphere.eu/zh/blogs/kubesphere-release-note-post/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/kubesphere-release-note-post/</guid>
      <description>农历二月二，KubeSphere 开源社区激动地向大家宣布，KubeSphere 容器平台 2.1.1 正式发布！
KubeSphere 作为 开源的企业级容器平台，对 2.1.1 版本定义的是 进一步增强生产可用性，修复了多个组件的 Bug，升级了内置的多个开源组件。借助 KubeSphere，您可以快速安装与管理原生的 Kubernetes，KubeSphere 2.1.1 已支持至 Kubernetes 1.17，帮助您上手 Kubernetes 新版本中新增的特性。并且，还向前兼容与支持 Kubernetes 1.17 之前的 3 个版本，您可以按需进行安装。
KubeSphere 能够帮助企业快速构建一个功能丰富的容器云平台。让企业在享受 Kubernetes 的弹性伸缩与敏捷部署的同时，还可以在容器平台拥有 IaaS 平台的存储与网络能力，获得与 IaaS 平台一样稳定的用户体验。比如，我们在 KubeSphere 2.1.1 新增了对阿里云与腾讯云块存储插件的集成，支持为 Pod 挂载公有云的存储，为有状态应用提供更稳定的持久化存储的能力。
除此之外，我们还将安装步骤再一次简化。2.1.1 简化了在已有 Kubernetes 上安装的步骤，无需再像 2.1.0 安装一样，配置集群 CA 证书路径。并且，也将 etcd 监控作为了可选安装项。真正实现了一条命令即可在已有的 Kubernetes 集群上快速安装 KubeSphere。
关于 2.1.1 的更新详情，请参考 Release Note。
下面演示两种最简单的安装方法，解锁如何最快尝鲜 KubeSphere 2.1.1。
如何在 Linux 快速安装 2.1.1  本文将演示 All-in-One 安装，请准备一台干净的机器（虚拟机或物理机），安装前关闭防火墙，并确保您的机器符合以下的最小要求：    机器配置:</description>
    </item>
    
    <item>
      <title>KubeSphere 部署 SkyWalking 至 Kubernetes 开启无侵入 APM</title>
      <link>https://kubesphere.eu/zh/blogs/skywalking-kubesphere/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/skywalking-kubesphere/</guid>
      <description>Kubernetes 天然适合分布式的微服务应用。然而，当开发者们将应用从传统的架构迁移到 Kubernetes 以后，会发现分布式的应用依旧存在各种各样的问题，例如大量微服务间的调用关系复杂、系统耗时或瓶颈难以排查、服务异常定位困难等一系列应用性能管理问题，而 APM 正是实时监控并管理微服务应用性能的利器。
为什么需要 APM APM 无疑是在大规模的微服务开发与运维场景下是必不可少的一环，APM 需要主要从这三个角度去解决三大场景问题：
 测试角度：性能测试调优监控总览，包括容器总体资源状况（如 CPU、内存、IO）与链路总体状况 研发角度：链路服务的细节颗粒追踪，数据分析与数据安全 运维角度：跟踪请求的处理过程，来对系统在前后端处理、服务端调用的性能消耗进行跟踪，实时感知并追踪访问体验差的业务  为什么选择 Apache SkyWalking 社区拥有很丰富的 APM 解决方案，比如著名的 Pinpoint、Zipkin、SkyWalking、CAT 等。在经过一番调研后，KubeSphere 选择将 Apache SkyWalking 作为面向 Kubernetes 的 APM 开源解决方案，将 Apache SkyWalking 集成到了 KubeSphere，作为应用模板在 KubeSphere 容器平台 提供给用户一键部署至 Kubernetes 的能力，进一步增强在微服务应用维度的可观察性。
Apache SkyWalking 在 2019 年 4 月 17 正式成为 Apache 顶级项目，提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。Apache SkyWalking 专为微服务、云原生和基于容器的架构而设计。这是 KubeSphere 选择 Apache SkyWalking 的主要原因。
并且，Apache SkyWalking 本身还具有很多优势，包括多语言自动探针，比如 Java、.NET Core 和 Node.JS，能够实现无侵入式的探针接入 APM 检测，轻量高效，多种后端存储支持，提供链路拓扑与 Tracing 等优秀的可视化方案，模块化，提供 UI、存储、集群管理多种机制可选，并且支持告警。同时，Apache SkyWalking 还很容易与 SpringCloud 应用进行集成。</description>
    </item>
    
    <item>
      <title>Maxnerva</title>
      <link>https://kubesphere.eu/zh/case/maxnerva/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/case/maxnerva/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NetApp 存储在 KubeSphere 上的实践</title>
      <link>https://kubesphere.eu/zh/blogs/netapp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/netapp/</guid>
      <description>NetApp 是向目前的数据密集型企业提供统一存储解决方案的居世界最前列的公司，其 Data ONTAP是全球首屈一指的存储操作系统。NetApp 的存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。 Ontap数据管理软件支持高速闪存、低成本旋转介质和基于云的对象存储等存储配置，为通过块或文件访问协议读写数据的应用程序提供统一存储。 Trident是一个由NetApp维护的完全支持的开源项目。以帮助您满足容器化应用程序的复杂持久性需求。 KubeSphere 是一款开源项目，在目前主流容器调度平台 Kubernetes 之上构建的企业级分布式多租户容器管理平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。
整体方案 在 VMware Workstation 环境下安装 ONTAP; ONTAP 系统上创建 SVM(Storage Virtual Machine) 且对接 nfs 协议；在已有 k8s 环境下部署 Trident,Trident 将使用 ONTAP 系统上提供的信息（svm、managementLIF 和 dataLIF）作为后端来提供卷；在已创建的 k8s 和StorageClass 卷下部署 KubeSphere。
版本信息  Ontap: 9.5 Trident: v19.07 k8s: 1.15 kubesphere: 2.0.2  步骤 主要描述ontap搭建及配置、Trident搭建和配置和kubesphere搭建及配置等方面。
OnTap 搭建及配置 在 VMware Workstation 上 Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide 运行，Ontap 启动之后，按下面操作配置，其中以 cluster base license、feature licenses for the non-ESX build 配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名: cluster1、密码等信息。</description>
    </item>
    
    <item>
      <title>OpenPitrix Insight</title>
      <link>https://kubesphere.eu/zh/blogs/openpitrix-insight/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/openpitrix-insight/</guid>
      <description>云计算在今天已经被绝大多数的企业所采用，具知名云服务厂商 RightScale 最近的调查显示，已经有越来越多的厂商采用多云管理。客户有太多的理由来选择多云管理了，其中最大的原因莫过于采用单一的供应商，会导致被锁定。因此，如何管理多云环境，并在多云的环境下进行自动化，正成为众多企业的刚需，而在这其中，应用程序的管理显得尤为的重要。进一步讲，颇具挑战的是创建一个一站式的应用管理平台，来管理不同类型的应用程序，其中包括传统的应用（或者称之为单体应用，或者传统的主从、分片、peer-to-peer 架构的企业分布式应用）、微服务应用、以及近来发展迅猛的 Serverless 应用等，OpenPitrix 就是为了解决这些问题而生的。用一句话来描述 OpenPitrix：
 OpenPitrix 是一款开源项目，用来在多云环境下打包、部署和管理不同类型的应用，包括传统应用、微服务应用以及 Serverless 应用等，其中云平台包括 AWS、Azure、Kubernetes、QingCloud、OpenStack、VMWare 等。
 微服务，即众所周知的微服务架构，这是程序设计的必然趋势，企业创建新的应用时选择的主要方式。另外，开源项目 Kubernetes 已经成为事实上的编排平台的领导者，其在自动化部署、扩展性、以及管理容器化的应用有着独特的优势。但是，仍然有大量的传统遗留应用用户想在毋须改变其架构的情况下迁入到云平台中，而且对很多用户来讲，采用微服务架构，或者是 Serverless 架构还是比较遥远的事情，所以，我们需要帮助这些用户将他们的传统应用迁入到云计算平台中，这也是 OpenPitrix 很重要的一个功能。
在2017年3月27日，QingCloud 发布 AppCenter，一款旨在为传统企业应用开发商和云用户之间架设友好桥梁的平台，该平台最大的亮点在于其可以让开发者以极低的学习成本就可以将传统的应用程序移植到 QingCloud 中运行，并且具有云计算的所有特性，如敏捷性、伸缩性、稳定性、监控等。通常，一位开发者只需花上几个小时就可以理解整个工作流程，然后，再花一到两周的时间(这具体要取决于应用的复杂性)将应用移植到云平台中。该平台上线之后一直颇受用户的青睐和夸赞，但有一些用户提出更多的需求，希望将之部署到他们内部来管理他们的多云环境。为了满足用户的需求，QingCloud 将之扩展，即在多云的环境下管理多种类型的应用程序，并且采用开源的方法来进行项目的良性发展。
俗语有云：&amp;ldquo;知易行难&amp;rdquo;，尽管 OpenPitrix 原始团队在云计算应用开发有着足够丰富的经验，并成功的开发出了稳定的商业化产品：AppCenter，要知道，等待在前方的依然有很多困难要克服。OpenPitrix 从一开始就是以开源的方式来进行，并且在2017年的8月份在 GitHub 上创建了组织和项目，一直到2018年2月24日才写下第一行功能代码，在此期间，团队的所有成员都在思考系统的每个关键点，这些讨论的细节均可在 GitHub 上公开访问。
以上便是 OpenPitrix 项目的来龙去脉介绍，接下来会解释一些详细的功能和设计细节。
主要的功能 OpenPitrix 所希望实现的功能包括以下内容：
 支持多个云平台，如 AWS、Azure、Kubernetes、QingCloud、OpenStack、VMWare 等等; 云平台的支持是高度可扩展和插拔的; 支持多种应用程序的类型：传统应用、微服务应用、Serverless 应用; 应用程序的支持也是高度可扩展的，这也就意味着无论将来出现哪种新的应用程序类型，OpenPitrix 平台都可以通过添加相应的插件来支持它; 应用程序的仓库是可配置的，这也就意味着由 OpenPitrix 所驱动的商店，其应用均是可以用来交易的; 应用程序库的可见性是可配置的，包括公开、私有或仅让某特定的一组用户可访问，由 OpenPitrix 所驱动的市场，每个供应商都能够操作属于她/他自己的应用商店。  用户场景实例 OpenPitrix 典型的用户场景有：
 某企业是采用了多云的系统（包括混合云），要实现一站式的应用管理平台，从而实现应用的部署和管理； 云管平台（CMP）可以将 OpenPitrix 视为其其中一个组件，以实现在多云环境下管理应用； 可以作为 Kubernetes 的一个应用管理系统。OpenPitrix 和 Helm 有着本质上的不同，虽然 OpenPitrix 底层用了 Helm 来部署 Kubernetes 应用，但 OpenPitrix 着眼于应用的全生命周期管理，比如在企业中，通常会按照应用的状态来分类，如开发、测试、预览、生产等；甚至有些组织还会按照部门来归类，而这是 Helm 所没有的。  架构概览 OpenPitrix 设计的最根本的思想就是解耦应用和应用运行时环境（此处使用运行时环境代替云平台，下同），如下图所示。应用程序能够运行在哪个环境，除了需要匹配 provider 信息之外，还需要匹配应用所在仓库的选择器 (selector) 和运行时环境的标签 (label)，即当某个最终用户从商店里选择了某个具体的应用，然后尝试部署它时，系统会自动选择运行时环境。如果有多个运行时环境可以运行此应用的话，则系统会弹出相应的对话框来让用户自行选择，更多设计细节请参考 OpenPitrix 设计文档。</description>
    </item>
    
    <item>
      <title>partner request</title>
      <link>https://kubesphere.eu/zh/partner/request/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/partner/request/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Porter-面向裸金属环境的 Kubernetes 开源负载均衡器</title>
      <link>https://kubesphere.eu/zh/conferences/porter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/conferences/porter/</guid>
      <description>我们知道，在 Kubernetes 集群中可以使用 “LoadBalancer” 类型的服务将后端工作负载暴露在外部。云厂商通常为 Kubernetes 提供云上的 LB 插件，但这需要将集群部署在特定 IaaS 平台上。然而，许多企业用户通常都将 Kubernetes 集群部署在裸机上，尤其是用于生产环境时。而且对于本地裸机集群，Kubernetes 不提供 LB 实施。Porter 是一个专为裸金属 Kubernetes 集群环境而设计的开源的负载均衡器项目，可完美地解决此类问题。
Kubernetes 服务介绍 在 Kubernetes 集群中，网络是非常基础也非常重要的一部分。对于大规模的节点和容器来说，要保证网络的连通性、网络转发的高效，同时能做的 IP 和 Port 自动化分配和管理，并提供给用户非常直观和简单的方式来访问需要的应用，这是非常复杂且细致的设计。
Kubernetes 本身在这方面下了很大的功夫，它通过 CNI、Service、DNS、Ingress 等一系列概念，解决了服务发现、负载均衡的问题，也大大简化了用户的使用和配置。其中的 Service 是 Kubernetes 微服务的基础，Kubernetes 是通过 kube-proxy 这个组件来实现服务的。kube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，通过管理 iptables 来实现网络的转发。用户可以创建多种形式的 Service，比如基于 Label Selector 、Headless 或者 ExternalName 的 Service，kube-proxy 会为 Service 创建一个虚拟的 IP（即 Cluster IP），用于集群内部访问服务。
暴露服务的三种方式 如果需要从集群外部访问服务，即将服务暴露给用户使用，Kubernetes Service 本身提供了两种方式，一种是 NodePort，另外一种是 LoadBalancer。另外 Ingress 也是一种常用的暴露服务的方式。
NodePort 如果将服务的类型设置为 NodePort，kube-proxy 就会为这个服务申请一个 30000 以上的端口号（默认情况下），然后在集群所有主机上配置 IPtables 规则，这样用户就能通过集群中的任意节点加上这个分配的端口号访问服务了，如下图</description>
    </item>
    
    <item>
      <title>VNG</title>
      <link>https://kubesphere.eu/zh/case/vng/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/case/vng/</guid>
      <description></description>
    </item>
    
    <item>
      <title>一文说清 KubeSphere 容器平台的价值</title>
      <link>https://kubesphere.eu/zh/blogs/kubesphere-values/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/kubesphere-values/</guid>
      <description>KubeSphere 作为云原生家族 后起之秀，开源近两年的时间以来收获了诸多用户与开发者的认可。本文通过大白话从零诠释 KubeSphere 的定位与价值，以及不同团队为什么会选择 KubeSphere。
对于企业 KubeSphere 是什么 KubeSphere 是在 Kubernetes 之上构建的 多租户 容器平台，以应用为中心，提供全栈的 IT 自动化运维的能力，简化企业的 DevOps 工作流。使用 KubeSphere 不仅能够帮助企业在公有云或私有化数据中心快速搭建 Kubernetes 集群，还提供了一套功能丰富的向导式操作界面。
KubeSphere 能够帮助企业快速构建一个功能丰富的容器云平台，让企业在享受 Kubernetes 的弹性伸缩与敏捷部署的同时，还可以在容器平台拥有 IaaS 平台的存储与网络能力，获得与 IaaS 一样稳定的用户体验。比如在 KubeSphere 2.1.1 新增了对阿里云与腾讯云块存储插件的集成，支持为 Pod 挂载公有云的存储，为有状态应用提供更稳定的持久化存储的能力。
在日常的运维开发中，我们可能需要使用与管理大量的开源工具，频繁地在不同工具的 GUI 和 CLI 窗口操作，每一个工具的单独安装、使用与运维都会带来一定的学习成本，而 KubeSphere 容器平台能够统一纳管与对接这些工具，提供一致性的用户体验。这意味着，我们不需要再去多线程频繁地在各种开源组件的控制面板窗口和命令行终端切换，极大赋能企业中的开发和运维团队，提高生产效率。
对于开发者 KubeSphere 是什么 有很多用户习惯把 KubeSphere 定义为 “云原生全家桶”。不难理解，KubeSphere 就像是一个一揽子解决方案，我们设计了一套完整的管理界面，开发与运维在一个统一的平台中，可以非常方便地安装与管理用户最常用的云原生工具，从业务视角提供了一致的用户体验来降低复杂性。为了不影响底层 Kubernetes 本身的灵活性，也为了让用户能够按需安装，KubeSphere 所有功能组件都是可插拔的。
KubeSphere 基于 OpenPitrix 和 Helm 提供了应用商店，对内可作为团队间共享企业内部的中间件、大数据、APM 和业务应用等，方便开发者一键部署应用至 Kubernetes 中；对外可作为根据行业特性构建行业交付标准、交付流程和应用生命周期管理的基础，作为行业通用的应用商店，可根据不同需求应对不同的业务场景。在 3.0 版本还将支持计量 (Metering)，方便企业对应用与集群资源消耗的成本进行管理。
对于运维 KubeSphere 是什么 可观察性是容器云平台非常关键的一环，狭义上主要包含监控、日志和追踪等，广义上还包括告警、事件、审计等。对于 Kubernetes 运维人员来说，通常需要搭建和运维一整套可观察性的技术架构，例如 Prometheus + Grafana + AlertManager、EFK 等等。并且，企业通常还需要对不同租户能够看到的监控、日志、事件、审计等信息，实现按不同租户隔离，这些需求的引入无疑会增大企业的运维成本与复杂性。</description>
    </item>
    
    <item>
      <title>云原生可观察性之日志管理</title>
      <link>https://kubesphere.eu/zh/conferences/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/conferences/logging/</guid>
      <description>日志通常含有非常有价值的信息，日志管理是云原生可观察性的重要组成部分。不同于物理机或虚拟机，在容器与 Kubernetes 环境中，日志有标准的输出方式(stdout)，这使得进行平台级统一的日志收集、分析与管理水到渠成，并体现出日志数据独特的价值。本文将介绍云原生领域比较主流的日志管理方案 EFK 、 KubeSphere 团队开发的 FluentBit Operator 以及 KubeSphere 在多租户日志管理方面的实践。此外还将介绍受 Prometheus 启发专为 Kubenetes 日志管理开发，具有低成本可扩展等特性的开源软件 Loki。
什么是可观察性 近年来随着以 Kubernetes 为代表的云原生技术的崛起，可观察性 ( Observability ) 作为一种新的理念逐渐走入人们的视野。云原生基金会 ( CNCF ) 在其 Landscape 里已经将可观察性单独列为一个分类，狭义上主要包含监控、日志和追踪等，广义上还包括告警、事件、审计等。在此领域陆续涌现出了众多新兴开源软件如 Prometheus, Grafana, Fluentd, Loki, Jaeger 等。
日志作为可观察性的重要组成部分在开发、运维、测试、审计等过程中起着非常重要的作用。著名的应用开发十二要素中提到：“日志使得应用程序运行的动作变得透明，应用本身从不考虑存储自己的输出流。 不应该试图去写或者管理日志文件。每一个运行的进程都会直接输出到标准输出（stdout）。每个进程的输出流由运行环境截获，并将其他输出流整理在一起，然后一并发送给一个或多个最终的处理程序，用于查看或是长期存档。”
在物理机或者虚拟机的环境中，日志通常是输出到文件，并由用户自己管理，这使得日志的集中管理和分析变得困难和不便。而 Kubernetes 、docker 等容器技术直接将日志输出到 stdout，这使得日志的集中管理和分析变得更为便捷和水到渠成。
Kubernetes 官网文档给出的通用日志架构如下图所示，包含日志 Agent，后端服务和前端控制台等三个部分。无论是成熟的日志解决方案如 ELK/EFK , 还是云原生领域 2018 年开源的 Loki 都具有相似的架构，下面将分别介绍 ELK/EFK , Loki 以及 KubeSphere) 在这方面的贡献。
新旧势力的联姻：从 ELK 到 EFK，从 Fluentd 到 Fluent Bit ELK 是 Elasticsearch, Logstash, Kibana 的简称，是目前比较主流的开源日志解决方案。 而 2019 年 4 月从 CNCF 毕业用 C 和 Ruby 编写的 Fluentd 作为通用日志采集器，以其高效、灵活、易用的特性逐渐取代了用 Java 编写的 Logstash 成为新的日志解决方案 EFK 中的重要一员，并在云原生领域得到广泛认可与应用。Google 的云端日志服务 Stackdriver 也用修改后的 Fluentd 作为 Agent 。然而 Fluentd 开发团队并没有停滞不前，推出了更为轻量级的完全用 C 编写的产品 Fluent Bit，两者的对比如下图所示：</description>
    </item>
    
    <item>
      <title>使用 KubeSphere DevOps 搭建自动化测试系统</title>
      <link>https://kubesphere.eu/zh/blogs/devops-automatic-testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/devops-automatic-testing/</guid>
      <description>测试分层 测试的目的是为了验证预期的功能，发现潜在的缺陷。测试增强了交付合格产品的信心，也给敏捷迭代带来了可能。可以说，测试决定了产品的开发进度。
网络模型有七层的 OSI 、四层的 TCP，而开发模式有 MTV、MVC、MVP、MVVM 等。高内聚、低耦合，划分职责、分模块、分层。然后结构化、标准化，技术逐步走向成熟。
测试也分为，UI 测试、API 测试、单元测试。测试并不是一项新技术，更多是产出与成本的一种平衡。
如上图，是一个测试金字塔。越往上，需要的成本越高，对环境要求越高，执行时间越长，维护越麻烦，但更贴近终端用户的场景。在 《Google软件测试之道》中，按照谷歌的经验，各层测试用例比例是 70：20：10，也就是 70% 的单元测试，20% 的 API 测试，10% 的 UI 测试。
本篇主要讲的是如何在 KubeSphere 平台上使用 KubeSphere DevOps 系统 运行自动化测试。
什么是 KubeSphere DevOps KubeSphere 针对容器与 Kubernetes 的应用场景，基于 Jenkins 提供了一站式 DevOps 系统，包括丰富的 CI/CD 流水线构建与插件管理功能，还提供 Binary-to-Image（B2I）、Source-to-Image（S2I），为流水线、S2I、B2I 提供代码依赖缓存支持，以及代码质量管理与流水线日志等功能。
KubeSphere 内置的 DevOps 系统将应用的开发和自动发布与容器平台进行了很好的结合，还支持对接第三方的私有镜像仓库和代码仓库形成完善的私有场景下的 CI/CD，提供了端到端的用户体验。
但是，很少有用户知道，KubeSphere DevOps 还可以用来搭建自动化测试系统，为自动化的单元测试、API 测试和 UI 测试带来极大的便利性，提高测试人员的工作效率。
单元测试 单元测试的运行频率非常高，每次提交代码都应该触发一次。单元测试的依赖少，通常只需要一个容器运行环境即可。
下面是一个使用 golang:latest 跑单元测试的例子。
pipeline { agent { node { label &#39;go&#39; } } stages { stage(&#39;testing&#39;) { steps { container(&#39;go&#39;) { sh &#39;&#39;&#39; git clone https://github.</description>
    </item>
    
    <item>
      <title>使用 KubeSphere 在 Kubernetes 安装 cert-manager 为网站启用 HTTPS</title>
      <link>https://kubesphere.eu/zh/blogs/install-cert-managner-on-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/install-cert-managner-on-k8s/</guid>
      <description>什么是 cert-manager cert-manager（https://cert-manager.io/）是 Kubernetes 原生的证书管理控制器。它可以帮助从各种来源颁发证书，例如 Let&amp;rsquo;s Encrypt，HashiCorp Vault，Venafi，简单的签名密钥对或自签名。它将确保证书有效并且是最新的，并在证书到期前尝试在配置的时间续订证书。它大致基于 kube-lego 的原理，并从其他类似项目（例如 kube-cert-manager）中借鉴了一些智慧。
准备工作  需要一个公网可访问的 IP，例如 139.198.121.121 需要一个域名，并且已经解析到到对应的IP，例如  A kubesphere.io 139.198.121.121，我们将 staging.kubesphere.io 域名解析到了 139.198.121.121 在KubeSphere上已经运行网站对应的服务，例如本例中的ks-console  启用项目网关 登录 KubeSphere，进入任意一个企业空间下的项目中。
在 KubeSphere 中启用对应项目下的网关。
 我们开启的是一个NodePort类型的网关，需要在集群外部使用 LoadBalancer 转发到网关的端口，将 139.198.121.121 绑定到 LoadBalancer 上，这样我们就可以通过公网IP直接访问我们的服务了； 如果 Kubernetes 集群是在物理机上，可以安装 Porter（https://porter.kubesphere.io）负载均衡器对外暴露集群服务； 如果在公有云上，可以安装和配置公有云支持的负载均衡器插件，然后创建 LoadBalancer 类型的网关，填入公网IP对应的 eip，会自动创建好负载均衡器，并将端口转发到网关。
 安装 cert-manager 详细安装文档可以参考 cert-manager。
 cert-manager 部署时会创建一个 webhook 来校验 cert-manager 相关对象是否符合格式，不过也会增加部署的复杂性。这里我们使用官方提供的一个 no-webhook 版本安装。
 可以在 KubeSphere 右下角的工具箱中，打开 Web Kubectl。
在 Web Kubectl 执行下列命令安装 cert-manager：</description>
    </item>
    
    <item>
      <title>在 KubeSphere 安装 Orion vGPU 使用 TensorFlow 运行深度学习训练</title>
      <link>https://kubesphere.eu/zh/blogs/kubesphere-orion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/kubesphere-orion/</guid>
      <description>概览 本文将使用 KubeSphere 容器平台，在 Kubernetes 上部署 Orion vGPU 软件 进行深度学习加速，并基于 Orion vGPU 软件使用经典的 Jupyter Notebook 进行模型训练与推理。
在开始安装 Orion vGPU 和演示深度学习训练之前，先简单了解一下，什么是 vGPU 以及什么是 Orion vGPU。
什么是 vGPU vGPU 又称 虚拟 GPU，早在几年前就由 NVIDIA 推出了这个概念以及相关的产品。vGPU 是通过对数据中心（物理机）的 GPU 进行虚拟化，用户可在多个虚拟机或容器中 共享该数据中心的物理 GPU 资源，有效地提高性能并降低成本。vGPU 使得 GPU 与用户之间的关系不再是一对一，而是 一对多。
为什么需要 vGPU 随着 AI 技术的快速发展，越来越多的企业开始将 AI 技术应用到自身业务之中。目前，云端 AI 算力主要由三类 AI 加速器来提供：GPU，FPGA 和 AI ASIC 芯片。这些加速器的优点是性能非常高，缺点是 成本高昂，缺少异构加速管理和调度。大部分企业因无法构建高效的加速器资源池，而不得不独占式地使用这些昂贵的加速器资源，导致 资源利用率低，成本高。
以 GPU 为例，通过创新的 vGPU 虚拟化技术，能够帮助用户无需任务修改就能透明地共享和使用数据中心内任何服务器之上的 AI 加速器，不但能够帮助用户提高资源利用率，而且可以 极大便利 AI 应用的部署，构建数据中心级的 AI 加速器资源池。</description>
    </item>
    
    <item>
      <title>基于 CSI Kubernetes 存储插件的开发实践</title>
      <link>https://kubesphere.eu/zh/conferences/csi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/conferences/csi/</guid>
      <description>现在很多用户都会将自己的应用迁移到 Kubernetes 容器平台中。在 Kubernetes 容器平台中，存储是支撑用户应用的基石。随着用户不断的将自己的应用深度部署在 K8S 容器平台中，但是我们现有的 Kubernetes 存储插件无论从多样性还是存储的功能来说，都无法满足用户日益增长的需求。我们急需开发新的存储插件，将我们的存储服务和 Kubernetes 容器平台相对接。
Kubernetes 存储插件分类 今天的主题是基于 CSI Kubernetes 存储插件的开发实践，我们会通过以下四部分为大家详细讲解 CSI 插件有什么功能，如何部署一个 CSI 插件，以及 CSI 的实践原理。
首先，我们会介绍 Kubernetes 存储插件的分类情况；然后为大家介绍如何开发一款 QingCloud 云平台 CSI 插件；之后，会介绍如何将 QingCloud 云平台 CSI 插件部署到 Kubernetes 容器平台中；最后，介绍如何对开发的 CSI 插件进行质量管理。
在 Kubernetes 容器平台中，Kubernetes 可以调用某类存储插件，对接后端存储服务，如调用 GCE 存储插件对接后端 GCE 存储服务。Kubernetes 里的存储插件可以分为 In-tree 和 Out-of-tree 这两大类。
首先，In-tree 存储插件的代码是在 Kubernetes 核心代码库里，In-tree 存储插件运行在 Kubernetes 核心组件里。Kubernetes 容器平台要使用后端某类存储服务，需要调用相应的 In-tree 存储插件，比如 Kubernetes 容器平台要使用后端 AWS 存储服务，需要调用 In-tree AWS 存储插件才能对接后端 AWS 存储服务。</description>
    </item>
    
    <item>
      <title>基于 Kubernetes 的 CI/CD 利器 — Prow 入门指南</title>
      <link>https://kubesphere.eu/zh/blogs/prow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/prow/</guid>
      <description>Prow是k8s使用的CI/CD系统(https://github.com/kubernetes/test-infra/tree/master/prow)，用于管理k8s的issue和pr。如果你经常去k8s社区查看pr或者提交过一些Pr后，就会经常看到一个叫k8s-ci-bot的机器人在各个Pr中回复，并且还能合并pr。在k8s-ci-bot中背后工作的就是Prow。Prow是为了弥补github上一些功能上的缺陷，它也是Jenkins-X的一部分，它具备这些功能：
 执行各种Job，包括测试，批处理和制品发布等，能够基于github webhook配置job执行的时间和内容。 一个可插拔的机器人功能（Tide），能够接受/foo这种样式的指令。 自动合并Pr 自带一个网页，能够查看当前任务的执行情况以及Pr的状况，也包括一些帮助信息 基于OWNER文件在同一个repo里配置模块的负责人 能够同时处理很多repo的很多pr 能够导出Prometheus指标  Prow拥有自己的CI/CD系统，但是也能与我们常见的CI/CD一起协作，所以如果你已经习惯了Jenkins或者travis，都可以使用Prow。
安装指南  官方repo提供了一个基于GKE快速安装指南，本文将基于青云的Iaas搭建Prow环境。不用担心，其中大部分步骤都是平台无关的，整个安装过程能够很方便的在其他平台上使用。
 一、 准备一个kubernetes集群 有以下多种方式准备一个集群
 利用kubeadm自建集群 在青云控制台上点击左侧的容器平台，选择其中的QKE，简单设置一些参数之后，就可以很快创建一个kubernetes集群。 将集群的kubeconfig复制到本地，请确保在本地运行kubectl cluster-info正确无误  二、 准备一个github机器人账号  如果没有机器人账号，用个人账号也可以。机器人账号便于区分哪些Prow的行为，所以正式使用时应该用机器人账号。
   在想要用prow管理的仓库中将机器人账号设置为管理员。
  在账号设置中添加一个[personal access token][1]，此token需要有以下权限：
 必须：public_repo 和 repo:status 可选：repo假如需要用于一些私有repo 可选：admin_org:hook 如果想要用于一个组织    将此Token保存在文件中，比如${HOME}/secrets/oauth
  用openssl rand -hex 20生成一个随机字符串用于验证webhook。将此字符串保存在本地，比如${HOME}/secrets/h-mac
  注意最后两步创建的token一定需要保存好，除了需要上传到k8s，后续配置也要用到，用于双向验证
三、 配置k8s集群  这里使用的default命名空间配置prow，如果需要配置在其他命名空间，需要在相关kubectl的命令中配置-n参数，并且在部署的yaml中配置命名空间。 建议将本repo克隆到本地，这个repo带有很多帮助配置Prow的小工具。
  将上一步中创建token和hmac保存在k8s集群中  # openssl rand -hex 20 &amp;gt; ${HOME}/secrets/h-mac kubectl create secret generic hmac-token --from-file=hmac=${HOME}/secrets/h-mac kubectl create secret generic oauth-token --from-file=oauth=${HOME}/secrets/oauth 部署Prow。由于Prow官方yaml中使用了grc.</description>
    </item>
    
    <item>
      <title>基于 Kubernetes 的 Serverless Jenkins —  Jenkins X</title>
      <link>https://kubesphere.eu/zh/conferences/jenkins-x/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/conferences/jenkins-x/</guid>
      <description>在云原生时代，应用模块不断被拆分，使得模块的数量不断上涨并且关系也越加复杂。企业在落地云原生技术的时候同事也需要有强大的 DevOps 手段，没有 DevOps 的云原生不可能是成功的。Jenkins X 是 CDF（持续交付基金会）与 Jenkins 社区在云原生时代的 DevOps产品，本文我们将介绍 Jenkins X 以及 Jenkins X 背后的技术。
背景 Jenkins 在2004年诞生。根据官网的数据统计（截止2019年3月）有 250,000 的 Jenkins 服务器正在运行、15,000,000+ Jenkins 用户、1000+ Jenkins插件。Jenkins 在 DevOps 领域取得了巨大的成功，但随着技术的不断发展与用户数量的不断上升，传统 Jenkins 所暴露出来的问题也越来越多。在这里我们将介绍传统 Jenkins 所遇到的挑战。
Jenkins 所遇到的挑战 - 单点故障 在传统的 Jenkins 当中，我们首先会遇到的问题就是 Jenkins 的单点故障问题。 Jenkins 的历史非常悠久，在当时大多数程序都是单机程序，Jenkins也不例外。 对比其他系统，Jenkins 的单点故障问题会更加凸显，熟悉 Jenkins 的用户都知道，它是一个基于插件的系统，而我们会经常安装插件，这时候我们就需要重启 Jenkins 服务器。这将导致共用这个平台的所有用户都无法使用。
Jenkins 所遇到的挑战 - JVM消耗资源多 Jenkins 是 Java 系的程序，这使得 Jenkins 需要使用 JVM，而 JVM 将会消耗大量的内存。 CI/CD 任务往往都是在代码提交时被触发，在非工作时间，这些资源消耗是可以大大降低的。
Jenkins 所遇到的挑战 - Job 的调度方式使 CI/CD 变得困难 在 Jenkins 诞生的年代，机器资源并没有像现在一样丰富、可调度，导致 Jenkins 的调度模式使得不适合现代的环境。 Jenkins 的调度模式是一种尽量能够节省资源的方式进行调度的。在一般的调度过程中 Jenkins 需要经历以下几个阶段: 检查有没有可用的 agent -&amp;gt; 如果没有的可用的agent，计算是否有 agent 预计将要运行完任务 -&amp;gt; 等待一段时间-&amp;gt; 启动动态的 agent -&amp;gt; agent 与 master建立连接。 这种方式使 CI/CD 任务被执行的太慢，我们往往都需要等待几十秒甚至更长时间来准备 CI/CD的执行环境。</description>
    </item>
    
    <item>
      <title>基于 Kubernetes 部署 node.js APP</title>
      <link>https://kubesphere.eu/zh/blogs/nodejs-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/nodejs-app/</guid>
      <description>什么是 Kubernetes Kubernetes 是一个开源容器编排引擎，可以帮助开发者或运维人员部署和管理容器化的应用，能够轻松完成日常开发运维过程中诸如 滚动更新，横向自动扩容，服务发现，负载均衡等需求。了解更多
安装 Kubernetes 可以通过快速安装 kubernetes 集群：
 KubeSphere Installer minikube kubeadm  Kubernetes 术语介绍 Pod Pod 是 Kubernetes 最小调度单位，是一个或一组容器的集合。
Deployment 提供对 Pod 的声明式副本控制。指定 Pod 模版，Pod 副本数量, 更新策略等。
Service Service 定义了 Pod 的逻辑分组和一种可以访问它们的策略。借助Service，应用可以方便的实现服务发现与负载均衡。
Label &amp;amp; Selector Kubernetes 中使用 Label 去关联各个资源。
 通过资源对象(Deployment, etc.)上定义的 Label Selector 来筛选 Pod 数量。 通过 Service 的 Label Selector 来选择对应的 Pod， 自动建立起每个 Service 到对应 Pod 的请求转发路由表。 通过对某些 Node 定义特定的 Label，并且在 Pod 中添加 NodeSelector 属性，可以实现 Pod 的定向调度(运行在哪些节点上)。  Nodejs 模板项目 node-express-realworld-example-app 是一款 node.</description>
    </item>
    
    <item>
      <title>微服务进阶之路 容器落地避坑指南</title>
      <link>https://kubesphere.eu/zh/blogs/microservice-blog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/microservice-blog/</guid>
      <description>微服务架构相对于单体架构有很大的变化，也产生了一些新的设计模式，比如 sidecar，如何开发一个微服务应用是一件有很大挑战性的事情，我们经常会听到有人讨论如何划分微服务，多细的颗粒度才是微服务等问题。初学者经常会处于一个“忐忑不安”的状态，所以我们急需要知道如何才能走上正确的微服务道路，或者需要一些最佳实践指导我们如何设计、开发一个微服务应用。
不骄不躁不跟风 知己知彼方可百战不殆 虽然现在已经进入到一个不谈微服务就落伍的时代，但作为 IT 从业者，我们一定要站在切身利益出发，多思考几个“为什么”，不要急于跟风。原因很简单，不管外面如何风吹雨打，只要你的房子足够结实、安全、舒服，那一般情况下就不需要拆除重建，所以在决定继续沿用单体架构还是转向微服务架构之前，我们一定要做两件事情：
第一件事，从外部了解两种架构各自的优劣： 可以看到，单体应用并不是一无是处。
第二件事，审视我们自己的业务：  上述单体架构列出的一些问题是否已经严重影响了我们的业务？ 企业新的业务系统是否要满足快速迭代、弹性等需求？ 团队内是否有 DevOps 氛围？ 企业内是否有足够的动力和技术储备去接触新的技术？  了解了单体应用和微服务应用的优劣特点，分析了企业自身的业务诉求和实际情况，最终还是决定转型微服务架构，那么我们也要清楚这不是一朝一夕的事情，需要分阶段逐步推进。
蒙眼狂奔不可取 循序渐进方可顺利进阶 第一阶段试炼—— 开发新应用 对于初次接触微服务的企业，选择新应用入手是正确的方式。
第一步可以选择 web-scale、无状态类型的新应用上手，比如基于 nginx 的网站、文档等，这类应用非常简单且容易实现，而且能体验到微服务在容器平台上的各种功能。
有了一定的经验之后，第二步就可以开发有状态类型的新应用，有状态服务的最大挑战就是数据管理。
敲重点，跟以往单体应用的共享数据库不同，微服务应用中的每一个服务“独享”自己的数据库，服务之间需要通过 API、事件或消息传递的方式来相互访问对方的数据，而不是通过直接访问对方数据库的方式。
换句话说，理想中的微服务是封装自己的数据，通过API暴露数据出去，从而避免数据耦合，这样每个微服务的数据格式发生变化也不影响其它微服务的数据调用。开发过和升级过大型企业单体应用的人对此会深有体会，一旦有人改变了数据库 schema，整个应用都有可能启动不起来，团队开发效率会大大降低。
微服务架构并不尽善尽美，适合自己的方案才是王道。
不难理解，微服务数据是牺牲强一致性而通过最终一致性的方式来管理，这对数据的划分带来很大难度，比如不能再用 join 的方式访问不同服务之间的数据表，实际当中也比较难做到或者做起来很麻烦，现在也没有成熟且好用的库或框架提供微服务的数据管理，而且某些应用确实需要强一致性。
而此时，我们不能通盘否定此类应用微服务化的可行性，应该适当折中或“妥协”，采用 miniservice。
Miniservice 在开发与部署的独立性和敏捷性方面类似于微服务(microservice)，但没有微服务那么强的约束。通常情况下，一个 miniservcie 可以提供多个功能，这些功能之间可以共享数据库。这个时候千万不要害怕混合架构，不要害怕自己的微服务应用是否“正统”，“think big，start small，move fast“才是我们应该遵循的哲学。
因此，一个企业应用里既有 microservice 也有 miniservice，甚至有单体部分（可以称之为 macroservice）都是可以接受的。
以一个电商平台举例，在整个场景里面，业务开发人员面对的主要压力来自前端频繁的变动，因为要应对频繁的促销、推广、降价等活动，所以面对消费者最前端的业务需要快速迭代。消费者会不停的浏览商品，最终产生交易的请求数量要远低于获取商品信息的请求数量，因此将前端业务无状态化，进行微服务拆分、解耦，便可以快速应对市场变化，灵活做出改变。
那是不是把整个平台都做到微服务级别会变得更好？答案是“不确定”，因为当微服务量级到达一定程度，由此产生的管理和运维压力是指数级增长的。而实际上，对于有些业务来讲也没有必要微服务化，比如很多电商平台都有 2B 的业务，其业务变化的频度和压力没有 2C 那么大，那以 macroservices 或者 miniservices 的方式去交付也是可以的。
开发人员应该分析在整个应用架构体系中，哪些适合微服务化，哪些亟需微服务化。
实践出真知 在上面的电商案例中，我们提到了服务无状态化，之所以期望服务无状态化，是因为无状态应用可以做到快速的扩缩容，可以应对井喷流量，可以最大效率的利用计算资源。
我们经常听到，以无状态为荣，以有状态为耻，说的就是对于一个服务要尽量无状态化它，比如用户 session 管理，以前我们在业务逻辑模块进行管理，导致这些模块不能按照无状态方式任意伸缩。我们可以把这些 session 的管理抽取出来放到一个高可用或分布式的缓存中管理，业务模块通过调用API的方式去获取 session，这样就实现了这些模块的无状态化。</description>
    </item>
    
    <item>
      <title>手把手从零部署与运营生产级的 Kubernetes 集群与 KubeSphere</title>
      <link>https://kubesphere.eu/zh/blogs/kubernetes-kubesphere-ha/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/kubernetes-kubesphere-ha/</guid>
      <description>本文来自 KubeSphere 社区用户 Liu_wt 投稿，欢迎所有社区用户参与投稿或分享经验案例。
 本文将从零开始，在干净的机器上安装 Docker、Kubernetes (使用 kubeadm)、Calico、Helm 与 KubeSphere，通过手把手的教程演示如何搭建一个高可用生产级的 Kubernetes，并在 Kubernetes 集群之上安装 KubeSphere 容器平台可视化运营集群环境。
一、准备环境 开始部署之前，请先确定当前满足如下条件，本次集群搭建，所有机器处于同一内网网段，并且可以互相通信。
⚠️⚠️⚠️：请详细阅读第一部分，后面的所有操作都是基于这个环境的，为了避免后面部署集群出现各种各样的问题，强烈建议你完全满足第一部分的环境要求
  两台以上主机 每台主机的主机名、Mac 地址、UUID 不相同 CentOS 7（本文用 7.6/7.7） 每台机器最好有 2G 内存或以上 Control-plane/Master至少 2U 或以上 各个主机之间网络相通 禁用交换分区 禁用 SELINUX 关闭防火墙（我自己的选择，你也可以设置相关防火墙规则） Control-plane/Master和Worker节点分别开放如下端口   Master节点
   协议 方向 端口范围 作用 使用者     TCP 入站 6443* Kubernetes API 服务器 所有组件   TCP 入站 2379-2380 etcd server client API kube-apiserver, etcd   TCP 入站 10250 Kubelet API kubelet 自身、控制平面组件   TCP 入站 10251 kube-scheduler kube-scheduler 自身   TCP 入站 10252 kube-controller-manager kube-controller-manager 自身    Worker节点</description>
    </item>
    
    <item>
      <title>本来生活的 DevOps 升级之路</title>
      <link>https://kubesphere.eu/zh/blogs/benlai-devops/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphere.eu/zh/blogs/benlai-devops/</guid>
      <description>我叫杨杨，就职于本来生活网（Benlai.com），负责发布系统架构。我们公司咋说呢，简单说就是卖水果、蔬菜的😄，下面还是来一段官方介绍。
本来生活简介 本来生活网创办于 2012 年，是一个专注于食品、水果、蔬菜的电商网站，从优质食品供应基地、供应商中精挑细选，剔除中间环节，提供冷链配送、食材食品直送到家服务。致力于通过保障食品安全、提供冷链宅配、基地直送来改善中国食品安全现状，成为中国优质食品提供者。
技术现状 基础设施   部署在 IDC 机房 拥有 100 多台物理机 虚拟化部署   存在的问题   物理机 95% 以上的占用率 相当多的资源闲置 应用扩容比较慢   拥抱 DevOps 与 Kubernetes 公司走上容器平台的 DevOps 这条康庄大道主要目标有三：
 1、提高资源利用率
2、提高发布效率
3、降低运维的工作成本等等
 其实最主要的还是 省钱，对就是 省钱。接下来就是介绍我们本来生活的 DevOps 升级之路：
Level 1：工具选型 我们从初步接触 DevOps 相关知识，在此期间偶然了解到开源的 KubeSphere (kubesphere.io)。KubeSphere 是在 Kubernetes 之上构建的以应用为中心的企业级容器平台，支持敏捷开发与自动化运维、DevOps、微服务治理、灰度发布、多租户管理、监控告警、日志查询与收集、应用商店、存储管理、网络管理等多种业务场景。
KubeSphere 内置的基于 Jenkins 的 DevOps 流水线非常适合我们，并且还打通了我们日常运维开发中需要的云原生工具生态，这个平台正是我们当初希望自己开发实现的。
于是，我们开始学习 KubeSphere 与 Jenkins 的各种操作、语法、插件等，开始构建适合我们自己的 CI/CD 的整个流程。最终结合 KubeSphere 容器平台，初步实现了第一级的 CI/CD 流程。</description>
    </item>
    
  </channel>
</rss>